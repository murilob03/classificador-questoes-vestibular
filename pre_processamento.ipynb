{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c5b0ba",
   "metadata": {},
   "source": [
    "## Inicialização\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1e1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"./data/questoes_unificadas.parquet\")\n",
    "df[\"texto_original\"] = df[\"enunciado\"].fillna(\"\") + \" \" + df[\"alternativas\"].fillna(\"\")\n",
    "df.dropna(subset=[\"enunciado\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8b660",
   "metadata": {},
   "source": [
    "## Limpar parte textual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f4868",
   "metadata": {},
   "source": [
    "- Aplica máscaras sobre números, símbolos pertinentes e URLs\n",
    "\n",
    "- Remove marcadores de alternativas\n",
    "\n",
    "- Remove pontuação\n",
    "\n",
    "- Cria uma coluna mantendo acentuação e uma removendo\n",
    "\n",
    "- Normaliza espaços\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe27909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "RE_URL = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "RE_OPTIONS_INLINE = re.compile(r\"\\s*[ABCDE]\\s*[\\)\\.\\-]\\s*\")\n",
    "RE_MULTI_SPACES = re.compile(r\"\\s+\")\n",
    "\n",
    "special_map = {\n",
    "    \"Δ\": \"<delta>\",\n",
    "    \"°\": \"<graus>\",\n",
    "    \"º\": \"<graus>\",\n",
    "    \"√\": \"<raiz>\",\n",
    "    \"π\": \"<pi>\",\n",
    "    \"Ω\": \"<ohm>\",\n",
    "    \"λ\": \"<lambda>\",\n",
    "    \"θ\": \"<theta>\",\n",
    "    \"μ\": \"<mu>\",\n",
    "    \"Σ\": \"<soma>\",\n",
    "    \"₀\": \"0\",\n",
    "    \"₁\": \"1\",\n",
    "    \"₂\": \"2\",\n",
    "    \"₃\": \"3\",\n",
    "    \"₄\": \"4\",\n",
    "    \"₅\": \"5\",\n",
    "    \"₆\": \"6\",\n",
    "    \"₇\": \"7\",\n",
    "    \"₈\": \"8\",\n",
    "    \"₉\": \"9\",\n",
    "    \"⁰\": \"0\",\n",
    "    \"¹\": \"1\",\n",
    "    \"²\": \"2\",\n",
    "    \"³\": \"3\",\n",
    "    \"⁴\": \"4\",\n",
    "    \"⁵\": \"5\",\n",
    "    \"⁶\": \"6\",\n",
    "    \"⁷\": \"7\",\n",
    "    \"⁸\": \"8\",\n",
    "    \"⁹\": \"9\",\n",
    "}\n",
    "\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_hyphens_quotes(s: str) -> str:\n",
    "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\").replace(\"´\", \"'\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_question(text: str, mask_numbers: bool = True):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # unicode + normalizações simples + aplicação de máscara para símbolos importantes\n",
    "    t = text\n",
    "    t = t.translate(str.maketrans(special_map))\n",
    "\n",
    "    t = t.replace(\"ı\", \"i\")\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = normalize_hyphens_quotes(t)\n",
    "\n",
    "    t = t.translate(str.maketrans(special_map))\n",
    "\n",
    "    t = t.casefold()\n",
    "\n",
    "    # máscaras para urls\n",
    "    t = RE_URL.sub(\" <url> \", t)\n",
    "\n",
    "    # remove marcadores de alternativas\n",
    "    t = RE_OPTIONS_INLINE.sub(\" \", t)\n",
    "\n",
    "    # remove marcadores de texto\n",
    "    t = re.sub(r\"\\btexto\\s+(?:i+|\\d+)\\b\", \"\", t)\n",
    "\n",
    "    # números e unidades\n",
    "    if mask_numbers:\n",
    "        # substitui números por token (mantém % e °celsius próximos)\n",
    "        t = re.sub(r\"(?<![a-zA-Z])\\d+[.,]?\\d*(?![a-zA-Z])\", \" <NUM> \", t)\n",
    "\n",
    "    # remove pontuação (mantém %, /, <, >  e -)\n",
    "    t = re.sub(r\"[^\\w\\s/%<>-]\", \" \", t)  # remove sinais exceto alguns\n",
    "\n",
    "    # colapsa espaços\n",
    "    t = RE_MULTI_SPACES.sub(\" \", t).strip()\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "# Limpa\n",
    "df[\"texto_clean\"] = df[\"texto_original\"].apply(clean_question)\n",
    "\n",
    "# Dropa duplicatas\n",
    "df.drop_duplicates(subset=[\"topico\", \"texto_clean\"], keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ce2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "RE_URL = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "RE_OPTIONS_INLINE = re.compile(r\"\\s*[ABCDE]\\s*[\\)\\.\\-]\\s*\")\n",
    "RE_MULTI_SPACES = re.compile(r\"\\s+\")\n",
    "\n",
    "special_map = {\n",
    "    \"Δ\": \"<delta>\",\n",
    "    \"°\": \"<graus>\",\n",
    "    \"º\": \"<graus>\",\n",
    "    \"√\": \"<raiz>\",\n",
    "    \"π\": \"<pi>\",\n",
    "    \"Ω\": \"<ohm>\",\n",
    "    \"λ\": \"<lambda>\",\n",
    "    \"θ\": \"<theta>\",\n",
    "    \"μ\": \"<mu>\",\n",
    "    \"Σ\": \"<soma>\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_hyphens_quotes(s: str) -> str:\n",
    "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\").replace(\"´\", \"'\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_question(text: str, mask_numbers: bool = True):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # unicode + normalizações simples + aplicação de máscara para símbolos importantes\n",
    "    t = text\n",
    "    t = t.translate(str.maketrans(special_map))\n",
    "\n",
    "    t = t.replace(\"ı\", \"i\")\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = normalize_hyphens_quotes(t)\n",
    "\n",
    "    t = t.translate(str.maketrans(special_map))\n",
    "\n",
    "    # máscaras para urls\n",
    "    t = RE_URL.sub(\" <url> \", t)\n",
    "\n",
    "    # remove marcadores de alternativas\n",
    "    t = RE_OPTIONS_INLINE.sub(\" \", t)\n",
    "\n",
    "    # remove marcadores de texto\n",
    "    t = re.sub(r\"\\btexto\\s+(?:i+|\\d+)\\b\", \"\", t)\n",
    "\n",
    "    # números e unidades\n",
    "    if mask_numbers:\n",
    "        # substitui números por token (mantém % e °celsius próximos)\n",
    "        t = re.sub(r\"(?<![a-zA-Z])\\d+[.,]?\\d*(?![a-zA-Z])\", \" <num> \", t)\n",
    "\n",
    "    # remove pontuação (mantém %, /, <, >  e -)\n",
    "    t = re.sub(r\"[^\\w\\s/%<>-]\", \" \", t)  # remove sinais exceto alguns\n",
    "\n",
    "    # colapsa espaços\n",
    "    t = RE_MULTI_SPACES.sub(\" \", t).strip()\n",
    "\n",
    "    # remove marcadores de texto (novamente, caso tenha sobrado algo)\n",
    "    t = re.sub(r\"\\btexto\\s+(?:i+|\\d+|<num>)\", \"\", t)\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "# Limpa\n",
    "df[\"texto_clean_cased\"] = df[\"texto_original\"].apply(clean_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048e512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# ------- STOPWORDS -------\n",
    "spacy.cli.download(\"pt_core_news_sm\")\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# stopwords padrão\n",
    "stopwords_spacy = set(nlp.Defaults.stop_words)\n",
    "\n",
    "# negações que queremos manter\n",
    "manter = {\n",
    "    \"mínimo\",\n",
    "    \"minimo\",\n",
    "    \"maximo\",\n",
    "    \"máximo\",\n",
    "    \"área\",\n",
    "    \"area\",\n",
    "    \"possível\",\n",
    "    \"possivel\",\n",
    "    \"ponto\",\n",
    "    \"pontos\",\n",
    "    \"não\",\n",
    "    \"nao\",\n",
    "    \"nem\",\n",
    "    \"nunca\",\n",
    "    \"jamais\",\n",
    "    \"sem\",\n",
    "}\n",
    "\n",
    "# remove essas negações da lista de stopwords\n",
    "stopwords_custom = stopwords_spacy - manter\n",
    "\n",
    "\n",
    "def batch_remove_stopwords(texts, stopwords_custom, lemmatize=False, batch_size=512):\n",
    "    result = []\n",
    "    with nlp.select_pipes(\n",
    "        disable=[\"parser\", \"ner\"]\n",
    "    ):  # desabilita partes que não são necessárias para esse processo\n",
    "        for doc in nlp.pipe(\n",
    "            texts, batch_size=batch_size, disable=[\"parser\", \"ner\"], n_process=8\n",
    "        ):  # processa em batches\n",
    "            tokens = [\n",
    "                tok.lemma_ if lemmatize else tok.text\n",
    "                for tok in doc\n",
    "                if tok.text not in stopwords_custom\n",
    "            ]\n",
    "            result.append(\" \".join(tokens))\n",
    "    return result\n",
    "\n",
    "\n",
    "remap = {\n",
    "    \"< delta >\": \"<delta>\",\n",
    "    \"< graus >\": \"<graus>\",\n",
    "    \"< graus >\": \"<graus>\",\n",
    "    \"< raiz >\": \"<raiz>\",\n",
    "    \"< pi >\": \"<pi>\",\n",
    "    \"< ohm >\": \"<ohm>\",\n",
    "    \"< lambda >\": \"<lambda>\",\n",
    "    \"< theta >\": \"<theta>\",\n",
    "    \"< mu >\": \"<mu>\",\n",
    "    \"< soma >\": \"<soma>\",\n",
    "    \"< NUM >\": \"<num>\",\n",
    "}\n",
    "\n",
    "\n",
    "# Função para consertar as máscaras no texto\n",
    "def conserta_mascaras(t: str) -> str:\n",
    "    for old, new in remap.items():\n",
    "        t = t.replace(old, new)\n",
    "    return t\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "df[\"texto_sw\"] = batch_remove_stopwords(df[\"texto_clean\"].to_list(), stopwords_custom)\n",
    "df[\"texto_sw\"] = df[\"texto_sw\"].apply(conserta_mascaras)\n",
    "\n",
    "df[\"texto_lem\"] = batch_remove_stopwords(\n",
    "    df[\"texto_clean\"].to_list(), stopwords_custom, lemmatize=True\n",
    ")\n",
    "df[\"texto_lem\"] = df[\"texto_lem\"].apply(conserta_mascaras)\n",
    "\n",
    "df[\"texto_clean\"] = df[\"texto_clean\"].apply(lambda x: x.replace(\"<NUM>\", \"<num>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5eaa3",
   "metadata": {},
   "source": [
    "### Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842b70db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/murilob/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RSLPStemmer  # Para português\n",
    "\n",
    "# Baixe o dicionário do nltk\n",
    "nltk.download(\"rslp\")\n",
    "\n",
    "# Escolha o stemmer adequado\n",
    "stemmer = RSLPStemmer()  # Para português\n",
    "\n",
    "\n",
    "def stemmer_function(text):\n",
    "    return \" \".join(stemmer.stem(word) for word in text.split())\n",
    "\n",
    "\n",
    "# Função para aplicar o stemming\n",
    "df[\"texto_stem\"] = df[\"texto_sw\"].apply(stemmer_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35e001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texto_clean\"] = df[\"texto_clean\"].astype(str).apply(lambda x: x.lower())\n",
    "df[\"texto_sw\"] = df[\"texto_sw\"].astype(str).apply(lambda x: x.lower())\n",
    "df[\"texto_lem\"] = df[\"texto_lem\"].astype(str).apply(lambda x: x.lower())\n",
    "df[\"texto_stem\"] = df[\"texto_stem\"].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "df[\"texto_clean\"] = (\n",
    "    df[\"texto_clean\"]\n",
    "    .astype(str)\n",
    "    .apply(lambda x: re.sub(r\"\\btexto\\s+(?:i+|\\d+|<num>)\", \"\", x))\n",
    ")\n",
    "df[\"texto_sw\"] = (\n",
    "    df[\"texto_sw\"]\n",
    "    .astype(str)\n",
    "    .apply(lambda x: re.sub(r\"\\btexto\\s+(?:i+|\\d+|<num>)\", \"\", x))\n",
    ")\n",
    "df[\"texto_lem\"] = (\n",
    "    df[\"texto_lem\"]\n",
    "    .astype(str)\n",
    "    .apply(lambda x: re.sub(r\"\\btexto\\s+(?:i+|\\d+|<num>)\", \"\", x))\n",
    ")\n",
    "df[\"texto_stem\"] = (\n",
    "    df[\"texto_stem\"]\n",
    "    .astype(str)\n",
    "    .apply(lambda x: re.sub(r\"\\btexto\\s+(?:i+|\\d+|<num>)\", \"\", x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa6bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mantém apenas as colunas desejadas\n",
    "df_final = df[\n",
    "    [\n",
    "        \"materia\",\n",
    "        \"topico\",\n",
    "        \"texto_clean\",\n",
    "        \"texto_clean_cased\",\n",
    "        \"texto_sw\",\n",
    "        \"texto_lem\",\n",
    "        \"texto_stem\",\n",
    "    ]\n",
    "]\n",
    "df_final = df_final.copy()\n",
    "df_final.dropna(subset=[\"texto_clean\"], inplace=True)\n",
    "df_final[\"id\"] = range(1, len(df_final) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20787f",
   "metadata": {},
   "source": [
    "## Salva\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18d93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_parquet(\"./data/questoes_tratadas.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
